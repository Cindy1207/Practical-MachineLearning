---
title: "ML_Assignment_Week4"
date: "10 oktober 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.width = 10, fig.height = 5)
library(caret)
library(ggplot2)
library(randomForest)
```
## Practical Machine Learning: Assignment Prediction  

### Executive summary

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E).
Goal of this project is to train a prediction model to predict the manner in which they did the exercise. This is the "classe" variable in the training set.
The prediction model will be used to predict 20 different test cases in the test data.

The data for this project come from <http://groupware.les.inf.puc-rio.br/har>.
The training data for this project are available here:
<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>
The test data are available here:
<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>


### Loading and Exploratory data analysis

*loading*

```{r loading data}
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
```

*exploratory data analysis*
```{r exploratory data analysis}
dim(training)
# str(training)
# head(training)
# summary(training)
```
The training dataset has 160 columns (159 features) which contain a lot of NA- and empty-values. These values will be removed. Also the zero covariates and the first 5 columns, used for identification, will be removed.

```{r cleaning the data}
# remove NA- and empty- values:
perc.max <- 10 # max number of rows with NA or empty-values allowed per column
row.max <- nrow(training) * perc.max/100
column.rm <- which(colSums(is.na(training) | training == "") > row.max)
training2 <- training[, -column.rm]
testing2 <- testing[, -column.rm]
# remove the zero covariates (variables which have no variability)
NZV <- nearZeroVar(training2)
training3 <- training2[ ,-NZV]
testing3 <- testing2[,-NZV]
# remove the first 5 columns with identifiers and time related features:
training4 <- training3[,-(1:5)]
testing4 <- testing3[,-(1:5)]
dim(training4)
```
After cleaning 54 columns are left. To visualize the dataset a barplot is created for the class variable "classe" to get a graphical representation of the class distribution. Class-A activity is the most frequently used activity.
```{r barplot for class breakdown}
plot(training4$classe)
```

Since the testing dataset provided will only be used for the quiz results generation, the training dataset will be partioned into a Training set (70% of the data) for the modeling process and a Test set (with 30% of the data) for the validations. 
```{r partition}
set.seed(246810)
inTrain <- createDataPartition(y=training4$classe, p = 0.7, list = FALSE)
trainSet <- training4[inTrain,]
testSet <- training4[-inTrain,]
```

### Prediction model building

For this supervised multiclass classification problem two methods can be applied to train the prediction model: Random Forest and Generalized Boosted Method (GBM). These methods are commonly used for these kind of problems. They deal well with possibly correlated predictors and high dimensional datasets. Below, Random Forest is applied. Within the training partition 7-fold cross validation will be used to improve the model fit. Experimenting with the number of trees for this problem shows that the error rate does not decline a lot after 50 trees. For this reason ntree=100 will be chosen:
```{r Random Forest}
set.seed(246810)
number = 7
control.Rf <- trainControl(method="cv", number, verboseIter=FALSE)
modfit.Rf <- train(classe ~., data = trainSet, method="rf", trControl = control.Rf, ntree = 100)
modfit.Rf$finalModel
```

```{r Prediction on testSet}
predRf.test <- predict(modfit.Rf, newdata=testSet)
confusionMatrix(testSet$classe, predRf.test)
ose <- 1 - as.numeric(confusionMatrix(testSet$classe, predRf.test)$overall[1])
ose
```
The expected out of sample error (ose) is 0.19%
The accuracy is very high: 0.9981. Because of this high accuracy the Generalized Boosted Method (GBM) will not be investigated.

### Applying the Random Forest model to get the quiz-answers

With the prediction model "modfit.Rf" predictions for the 20 test cases in "testing4" are made. The predictions will be used for the "Course Project Prediction Quiz Portion".

```{r Quiz answers}
predRf.testing4 <- predict(modfit.Rf, newdata = testing4)
predRf.testing4
```


